{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9b8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as kl\n",
    "from tensorflow.keras.initializers import VarianceScaling\n",
    "\n",
    "class ProbabilityDistribution(tf.keras.Model):\n",
    "    def call(self, logits, **kwargs):\n",
    "        # Random distribution\n",
    "        return tf.squeeze(tf.random.categorical(logits, 1), axis=-1)\n",
    "    \n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, num_actions, hidden):\n",
    "        # Note: no tf.get_variable(), just simple Keras API!\n",
    "        super().__init__('mlp_policy')\n",
    "        self.normalize = kl.Lambda(lambda layer: layer / 255)    # normalize by 255\n",
    "        self.conv1 = kl.Conv2D(32, (8, 8), strides=4, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)\n",
    "        self.conv2 = kl.Conv2D(64, (4, 4), strides=2, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)\n",
    "        self.conv3 = kl.Conv2D(64, (3, 3), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)\n",
    "        self.conv4 = kl.Conv2D(hidden, (7, 7), strides=1, kernel_initializer=VarianceScaling(scale=2.), activation='relu', use_bias=False)\n",
    "        \n",
    "        self.flatten = kl.Flatten()\n",
    "        self.value = kl.Dense(1, kernel_initializer=VarianceScaling(scale=2.), name=\"value\")\n",
    "        self.logits = kl.Dense(num_actions, kernel_initializer=VarianceScaling(scale=2.), name='policy_logits')\n",
    "        \n",
    "        self.dist = ProbabilityDistribution()\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # Inputs is a numpy array, convert to a tensor.\n",
    "        x = tf.convert_to_tensor(inputs)\n",
    "        # Separate hidden layers from the same input tensor.\n",
    "        x = self.normalize(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        return self.logits(x), self.value(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a0d616",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, save_path=PATH_SAVE_MODEL, load_path=PATH_LOAD_MODEL, lr=LR, gamma=GAMMA, value_c=VALUE_C,\n",
    "                 entropy_c=ENTROPY_C, clip_ratio=CLIP_RATIO, std_adv=STD_ADV, agent=AGENT, input_shape=INPUT_SHAPE,\n",
    "                 batch_size = BATCH_SIZE, updates=N_UPDATES):\n",
    "        # Coefficients are used for the loss terms.\n",
    "        self.value_c = value_c\n",
    "        self.entropy_c = entropy_c\n",
    "        # `gamma` is the discount factor\n",
    "        self.gamma = gamma\n",
    "        self.save_path = save_path\n",
    "        self.load_path = load_path\n",
    "        self.clip_ratio = clip_ratio\n",
    "        self.std_adv = std_adv\n",
    "        self.agent = agent\n",
    "        self.input_shape = input_shape\n",
    "        self.batch_size = batch_size\n",
    "        self.updates = updates\n",
    "        self.opt = opt.RMSprop(lr=lr)\n",
    "        \n",
    "        self.model = model\n",
    "        if load_path is not None:\n",
    "            print(\"loading model in {}\".format(load_path))\n",
    "            self.load_model(load_path)\n",
    "            print(\"model loaded\")\n",
    "            \n",
    "            \n",
    "    def train(self, wrapper):\n",
    "        # Storage helpers for a single batch of data.\n",
    "        actions = np.empty((self.batch_size), dtype=np.int32)\n",
    "        rewards, dones, values = np.empty((3, self.batch_size))\n",
    "        observations = np.empty((self.batch_size,) + self.input_shape)\n",
    "        old_logits = np.empty((self.batch_size, wrapper.env.action_space.n), dtype=np.float32)\n",
    "        # Training loop: collect samples, send to optimizer, repeat updates times.\n",
    "        ep_rewards = [0.0]\n",
    "        next_obs = wrapper.reset()\n",
    "        for update in tqdm(range(self.updates)):\n",
    "            start_time = time.time()\n",
    "            for step in range(self.batch_size):\n",
    "                observations[step] = next_obs.copy()\n",
    "                old_logits[step], actions[step], values[step] = self.logits_action_value(next_obs[None, :])\n",
    "                next_obs, rewards[step], dones[step] = wrapper.step(actions[step])\n",
    "                next_obs = wrapper.state\n",
    "                ep_rewards[-1] += rewards[step]\n",
    "                if dones[step]:\n",
    "                    ep_rewards.append(0.0)\n",
    "                    next_obs = wrapper.reset()\n",
    "                    wandb.log({'Game number': len(ep_rewards) - 1, '# Update': update, '% Update': round(update / self.updates, 2),\n",
    "                                \"Reward\": round(ep_rewards[-2], 2), \"Time taken\": round(time.time() - start_time, 2)})\n",
    "            _, _, next_value = self.logits_action_value(next_obs[None, :])\n",
    "            returns, advs = self._returns_advantages(rewards, dones, values, next_value, self.std_adv)\n",
    "            # Performs a full training step on the collected batch.\n",
    "            # Note: no need to mess around with gradients, Keras API handles it.\n",
    "            with tf.GradientTape() as tape:\n",
    "                logits, v = self.model(observations, training=True)\n",
    "                if self.agent == \"A2C\":\n",
    "                    logit_loss = self._logits_loss_a2c(actions, advs, logits)\n",
    "                elif self.agent == \"PPO\":\n",
    "                    logit_loss = self._logits_loss_ppo(old_logits, logits, actions, advs, wrapper.env.action_space.n)\n",
    "                else:\n",
    "                    raise Exception(\"Sorry agent can be just A2C or PPO\")\n",
    "                value_loss = self._value_loss(returns, v)\n",
    "                loss = logit_loss + value_loss\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "            if update % 5000 == 0 and self.save_path is not None:\n",
    "                print(\"Saving model in {}\".format(self.save_path))\n",
    "                self.save_model(f'{self.save_path}/save_agent_{time.strftime(\"%Y%m%d%H%M\") + \"_\" + str(update).zfill(8)}/model.tf')\n",
    "                print(\"model saved\")\n",
    "                \n",
    "        return ep_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146ca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _returns_advantages(self, rewards, dones, values, next_value, standardize_adv):\n",
    "        returns = np.append(np.zeros_like(rewards), next_value, axis=-1)\n",
    "        # Returns are calculated as discounted sum of future rewards.\n",
    "        for t in reversed(range(rewards.shape[0])):\n",
    "            returns[t] = rewards[t] + self.gamma * returns[t + 1] * (1 - dones[t])\n",
    "        returns = returns[:-1]\n",
    "        # Advantages are equal to returns - baseline (value estimates in our case).\n",
    "        advantages = returns - values\n",
    "        \n",
    "        if standardize_adv:\n",
    "            advantages = (advantages - np.mean(advantages)) / (np.std(advantages) + 1e-10)\n",
    "        return returns, advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55ba054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _value_loss(self, returns, value):\n",
    "        # Value loss is typically MSE between value estimates and returns.\n",
    "        return self.value_c * kloss.mean_squared_error(returns, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1acba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _logits_loss_a2c(self, actions, advantages, logits):\n",
    "        # Sparse categorical CE loss obj that supports sample_weight arg on `call()`.\n",
    "        # `from_logits` argument ensures transformation into normalized probabilities.\n",
    "        weighted_sparse_ce = kloss.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        # Policy loss is defined by policy gradients, weighted by advantages.\n",
    "        # Note: we only calculate the loss on the actions we've actually taken.\n",
    "        actions = tf.cast(actions, tf.int32)\n",
    "        policy_loss = weighted_sparse_ce(actions, logits, sample_weight=advantages)\n",
    "        # Entropy loss can be calculated as cross-entropy over itself.\n",
    "        probs = tf.nn.softmax(logits)\n",
    "        entropy_loss = kloss.categorical_crossentropy(probs, probs)\n",
    "        # We want to minimize policy and maximize entropy losses.\n",
    "        # Here signs are flipped because the optimizer minimizes.\n",
    "        return policy_loss - self.entropy_c * entropy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97b160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "  project=\"tensorflow2_pong_{}\".format(AGENT.lower()),\n",
    "  tags=[AGENT.lower(), \"CNN\", \"RL\", \"atari_pong\"],\n",
    "  config=CONFIG_WANDB,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af14cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pw = PongWrapper(ENV_NAME, history_length=4)\n",
    "model = Model(num_actions=pw.env.action_space.n, hidden=HIDDEN)\n",
    "agent = Agent(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bb9020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    rewards_history = agent.train(pw)\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        # Save the model, I need this in order to save the networks, frame number, rewards and losses. \n",
    "        # if I want to stop the script and restart without training from the beginning\n",
    "        if PATH_SAVE_MODEL is None:\n",
    "            print(\"Setting path to ../model/{}\".format(AGENT.lower()))\n",
    "            PATH_SAVE_MODEL = \"../model/{}\".format(AGENT.lower())\n",
    "        print('Saving the model in ' + f'{PATH_SAVE_MODEL}/save_agent_{time.strftime(\"%Y%m%d%H%M\")}')\n",
    "        agent.save_model(f'{PATH_SAVE_MODEL}/save_agent_{time.strftime(\"%Y%m%d%H%M\")}/model.tf')\n",
    "        print('Saved.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
