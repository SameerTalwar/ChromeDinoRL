{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv2\n",
    "from mss import mss\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import keyboard\n",
    "import time\n",
    "import tqdm as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf                                                               \n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Following Code Block is the Agent Class. It consists of the Convolutional NN that is the brains of the AI. It also contains the pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        #This is the actual Neural net\n",
    "        model = Sequential([ \n",
    "            Conv2D(32, (8,8), input_shape=(76, 384, 4),\n",
    "                   strides=(2,2), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(5,5), strides=(2, 2)),\n",
    "            Conv2D(64, (4,4), activation='relu', strides=(1,1)),\n",
    "            MaxPooling2D(pool_size=(7, 7), strides=(3, 3)),\n",
    "            Conv2D(128, (1, 1), strides=(1,1), activation='relu'),\n",
    "            MaxPooling2D(pool_size=(3,3), strides=(3,3)),\n",
    "            Flatten(),\n",
    "            Dense(384, activation='relu'),\n",
    "            Dense(64, activation=\"relu\", name=\"layer1\"),\n",
    "            Dense(8, activation=\"relu\", name=\"layer2\"),\n",
    "            Dense(3, activation=\"linear\", name=\"layer3\"),\n",
    "        ])\n",
    "        #pick your learning rate here\n",
    "        model.compile(loss='mean_squared_error', optimizer=Adam(lr=0.0001)) \n",
    "        #This is where you import your pretrained weights\n",
    "        model.load_weights(\"DinoGameSpeed4.h5\")\n",
    "        self.model = model\n",
    "        self.memory = []\n",
    "        # Print the model summary if you want to see what it looks like\n",
    "        # print(self.model.summary()) \n",
    "        self.xTrain = []\n",
    "        self.yTrain = []\n",
    "        self.loss = []\n",
    "        self.location = 0\n",
    "\n",
    "\n",
    "    def predict(self, state):\n",
    "        stateConv = state\n",
    "        qval = self.model.predict(np.reshape(stateConv, (1, 76, 384, 4)))\n",
    "        return qval\n",
    "\n",
    "    def act(self, state):\n",
    "        qval = self.predict(state)\n",
    "        #you can either pick softmax or epislon greedy actions.\n",
    "        #To pick Softmax, un comment the bottom 2 lines and delete everything below that \n",
    "        # prob = tf.nn.softmax(tf.math.divide((qval.flatten()), 1)) \n",
    "        # action = np.random.choice(range(3), p=np.array(prob))\n",
    "\n",
    "        #Epsilon-Greedy actions->\n",
    "        z = np.random.random()\n",
    "        epsilon = 0.004\n",
    "        if self.location > 1000:\n",
    "            epsilon = 0.05\n",
    "        epsilon = 0\n",
    "        if z > epsilon:\n",
    "            return np.argmax(qval.flatten())\n",
    "        else:\n",
    "            return np.random.choice(range(3))\n",
    "        return action\n",
    "\n",
    "    # This function stores experiences in the experience replay\n",
    "    def remember(self, state, nextState, action, reward, done, location):\n",
    "        self.location = location\n",
    "        self.memory.append(np.array([state, nextState, action, reward, done]))\n",
    "\n",
    "    #This is where the AI learns\n",
    "    def learn(self):\n",
    "        #Feel free to tweak this. This number is the number of experiences the AI learns from every round\n",
    "        self.batchSize = 256 \n",
    "\n",
    "        #If you don't trim the memory, your GPU might run out of memory during training. \n",
    "        #I found 35000 works well\n",
    "        if len(self.memory) > 35000:\n",
    "            self.memory = []\n",
    "            print(\"trimming memory\")\n",
    "        if len(self.memory) < self.batchSize:\n",
    "            print(\"too little info\")\n",
    "            return  \n",
    "        batch = random.sample(self.memory, self.batchSize)\n",
    "\n",
    "        self.learnBatch(batch)\n",
    "\n",
    "    #The alpha value determines how future oriented the AI is.\n",
    "    #bigger number (up to 1) -> more future oriented\n",
    "    def learnBatch(self, batch, alpha=0.9):\n",
    "        batch = np.array(batch)\n",
    "        actions = batch[:, 2].reshape(self.batchSize).tolist()\n",
    "        rewards = batch[:, 3].reshape(self.batchSize).tolist()\n",
    "\n",
    "        stateToPredict = batch[:, 0].reshape(self.batchSize).tolist()\n",
    "        nextStateToPredict = batch[:, 1].reshape(self.batchSize).tolist()\n",
    "\n",
    "        statePrediction = self.model.predict(np.reshape(\n",
    "            stateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        nextStatePrediction = self.model.predict(np.reshape(\n",
    "            nextStateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        statePrediction = np.array(statePrediction)\n",
    "        nextStatePrediction = np.array(nextStatePrediction)\n",
    "\n",
    "        for i in range(self.batchSize):\n",
    "            action = actions[i]\n",
    "            reward = rewards[i]\n",
    "            nextState = nextStatePrediction[i]\n",
    "            qval = statePrediction[i, action]\n",
    "            if reward < -5: \n",
    "                statePrediction[i, action] = reward\n",
    "            else:\n",
    "                #this is the q learning update rule\n",
    "                statePrediction[i, action] += alpha * (reward + 0.95 * np.max(nextState) - qval)\n",
    "\n",
    "        self.xTrain.append(np.reshape(\n",
    "            stateToPredict, (self.batchSize, 76, 384, 4)))\n",
    "        self.yTrain.append(statePrediction)\n",
    "        history = self.model.fit(\n",
    "            self.xTrain, self.yTrain, batch_size=5, epochs=1, verbose=0)\n",
    "        loss = history.history.get(\"loss\")[0]\n",
    "        print(\"LOSS: \", loss)\n",
    "        self.loss.append(loss)\n",
    "        self.xTrain = []\n",
    "        self.yTrain = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have the enviornment classs. This interacts with the actual chrome dino game and gathers screenshots for us. It then analyzes the screenshots and determines when the game is done. Make sure to tweak your screenshot view port in the __init__ function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Enviornment:\n",
    "    def __init__(self):\n",
    "        ########\n",
    "        #these are some various screenshot parameters that I found worked well for different resolutions\n",
    "        #Use it as a starting point but use the open cv code snippets below to tweak your screenshot window\n",
    "        # Do note that the lower the resolution you use, the faster the code runs\n",
    "        # I saw a 200% screenshot framerate increase from dropping my screen resolution from 4k to 720p\n",
    "\n",
    "        self.mon = {'top': 243, 'left': 0, 'width': 1366, 'height': 270} # 720p resolution\n",
    "        # self.mon = {'top': 380, 'left': 0, 'width': 1920, 'height': 380} #1080p resolution\n",
    "        # self.mon = {'top': 1000, 'left': 0, 'width': 3840, 'height': 760} #4k resolution\n",
    "        ########\n",
    "        \n",
    "        self.sct = mss()\n",
    "        self.counter = 0\n",
    "        self.startTime = -1\n",
    "        self.imageBank = []\n",
    "        self.imageBankLength = 4 #number of frames for the conv net\n",
    "        self.actionMemory = 2 #init as 2 to show no action taken   \n",
    "        #image processing\n",
    "        self.ones = np.ones((76,384,4))\n",
    "        self.zeros = np.zeros((76,384,4))  \n",
    "        self.zeros1 = np.zeros((76,384,4))\n",
    "        self.zeros2 = np.zeros((76,384,4))\n",
    "        self.zeros3 = np.zeros((76,384,4))\n",
    "        self.zeros4 = np.zeros((76,384,4))\n",
    "        self.zeros1[:,:,0] = 1\n",
    "        self.zeros2[:,:,1] = 1\n",
    "        self.zeros3[:,:,2] = 1\n",
    "        self.zeros4[:,:,3] = 1\n",
    "\n",
    "    def startGame(self):\n",
    "        #start the game, giving the user a few seconds to click on the chrome tab after starting the code\n",
    "        for i in reversed(range(3)):\n",
    "            print(\"game starting in \", i)\n",
    "            time.sleep(1)\n",
    "\n",
    "    def step(self, action):        \n",
    "        actions ={\n",
    "            0: 'space',\n",
    "            1: 'down'\n",
    "        }            \n",
    "        if action != self.actionMemory:\n",
    "            if self.actionMemory != 2:\n",
    "                keyboard.release(actions.get(self.actionMemory))\n",
    "            if action != 2:\n",
    "                keyboard.press(actions.get(action))\n",
    "        self.actionMemory = action\n",
    "\n",
    "        #This is where the screenshot happens\n",
    "        screenshot = self.sct.grab(self.mon)\n",
    "        img = np.array(screenshot)[:, :, 0]\n",
    "        processedImg = self._processImg(img)\n",
    "        state = self._imageBankHandler(processedImg)\n",
    "        done = self._done(processedImg)\n",
    "        reward = self._getReward(done)\n",
    "        return state, reward, done\n",
    "\n",
    "    def reset(self):\n",
    "        self.startTime = time.time()\n",
    "        keyboard.press(\"space\")\n",
    "        time.sleep(0.5)\n",
    "        keyboard.release(\"space\")\n",
    "        return self.step(0)\n",
    "\n",
    "    def _processImg(self, img):\n",
    "        img = Image.fromarray(img)\n",
    "        img = img.resize((384, 76), Image.ANTIALIAS)\n",
    "        if np.sum(img) > 2000000:\n",
    "            img = ImageOps.invert(img)\n",
    "        img = self._contrast(img)\n",
    "\n",
    "        #You can use the following open CV code segment to test your in game screenshots\n",
    "        # cv2.imshow(\"image\",img)\n",
    "        # if cv2.waitKey(25) & 0xFF == ord('q'): \n",
    "        #     cv2.destroyAllWindows()\n",
    "        \n",
    "        img = np.reshape(img, (76,384))\n",
    "        return img\n",
    "\n",
    "    def _contrast(self,pixvals):\n",
    "        minval = 32 #np.percentile(pixvals, 2)\n",
    "        maxval = 171 #np.percentile(pixvals, 98)\n",
    "        pixvals = np.clip(pixvals, minval, maxval)\n",
    "        pixvals = ((pixvals - minval) / (maxval - minval))\n",
    "        return pixvals\n",
    "\n",
    "    def _imageBankHandler(self, img):\n",
    "        img = np.array(img)\n",
    "        while len(self.imageBank) < (self.imageBankLength): \n",
    "            self.imageBank.append(np.reshape(img,(76,384,1)) * self.ones)\n",
    "\n",
    "        \n",
    "        bank = np.array(self.imageBank)\n",
    "        toReturn = self.zeros\n",
    "        img1 = (np.reshape(img,(76,384,1)) * self.ones)  * self.zeros1\n",
    "        img2 = bank[0] * self.zeros2\n",
    "        img3 = bank[1] * self.zeros3\n",
    "        img4 = bank[2] * self.zeros4\n",
    "\n",
    "\n",
    "        toReturn = np.array(img1 + img2 + img3 + img4)        \n",
    "\n",
    "        self.imageBank.pop(0)\n",
    "        self.imageBank.append(np.reshape(img,(76 ,384,1)) * self.ones)\n",
    "\n",
    "        return toReturn\n",
    "\n",
    "    def _getReward(self,done):\n",
    "        if done:\n",
    "            return -15\n",
    "        else: \n",
    "            return 1\n",
    "            return time.time() - self.startTime\n",
    "        \n",
    "    def _done(self,img):\n",
    "        img = np.array(img)\n",
    "        img  = img[30:50, 180:203]\n",
    "\n",
    "\n",
    "        val = np.sum(img)\n",
    "        #Sum of the reset pixels when the game ends in the night mode\n",
    "        expectedVal = 331.9352517985612 \n",
    "        #Sum of the reset pixels when the game ends in the day mode\n",
    "        expectedVal2 = 243.53\n",
    "\n",
    "        # This method checks if the game is done by reading the pixel values\n",
    "        # of the area of the screen at the reset button. Then it compares it to\n",
    "        # a pre determined sum. You might need to fine tune these values since each\n",
    "        # person's viewport will be different. use the following print statements to \n",
    "        # help you find the appropirate values for your use case \n",
    "\n",
    "        # print(\"val: \", val)\n",
    "        # print(\"Difference1: \", np.absolute(val-expectedVal2))\n",
    "        # print(\"Difference2: \", np.absolute(val-expectedVal))\n",
    "        if np.absolute(val-expectedVal) > 15 and np.absolute(val-expectedVal2) > 15: #seems to work well\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we have the learning loop. Here the Agent/AI is created, the enviornment wrapper is made, and then the AI plays the game. To actually start the game, make sure run this and then click on the chrome dinosaur game. The code gives you a 3 second (adjustable) buffer between the code starting and you shifting to the chrome game. This needs to be done since the AI is not in direct control of the chrome game but rather controlling it via emulating keyboard strokes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m plotX \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 3\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#currently agent is configured with only 2 actions\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     env \u001b[38;5;241m=\u001b[39m Enviornment()\n\u001b[0;32m      5\u001b[0m     env\u001b[38;5;241m.\u001b[39mstartGame()    \n",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39mAdam(lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m)) \n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#This is where you import your pretrained weights\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDinoGameSpeed4.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\files.py:567\u001b[0m, in \u001b[0;36mFile.__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[0;32m    558\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[0;32m    559\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[0;32m    560\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[0;32m    561\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[0;32m    562\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[0;32m    563\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    564\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[0;32m    565\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[0;32m    566\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[1;32m--> 567\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\h5py\\_hl\\files.py:231\u001b[0m, in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m swmr \u001b[38;5;129;01mand\u001b[39;00m swmr_support:\n\u001b[0;32m    230\u001b[0m         flags \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mACC_SWMR_READ\n\u001b[1;32m--> 231\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    233\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mopen(name, h5f\u001b[38;5;241m.\u001b[39mACC_RDWR, fapl\u001b[38;5;241m=\u001b[39mfapl)\n",
      "File \u001b[1;32mh5py\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\h5f.pyx:106\u001b[0m, in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "plotX = []\n",
    "while True:\n",
    "    agent = Agent() #currently agent is configured with only 2 actions\n",
    "    env = Enviornment()\n",
    "    env.startGame()    \n",
    "    #3500 refers to the number of episodes/iterations of the game to play\n",
    "    for i in tqdm(range(3500)): \n",
    "        state, reward, done = env.reset()\n",
    "        epReward = 0\n",
    "        done = False\n",
    "        episodeTime = time.time()\n",
    "        stepCounter = 0\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            nextState, reward, done = env.step(action)\n",
    "            ########\n",
    "            #This next section is storing more memory of later parts of the game since \n",
    "            #if you don't do this, most of the experience replay fills up with the \n",
    "            #starting parts of the game since its played more often. A more elegant \n",
    "            #approach to this is \"Prioritized experience replay\" but this is an effective\n",
    "            #alternative too\n",
    "            if stepCounter> 700:\n",
    "                for _ in range(5):\n",
    "                    agent.remember(state, nextState, action, reward, done, stepCounter)\n",
    "            elif stepCounter> 40:\n",
    "                agent.remember(state, nextState, action, reward, done, stepCounter)                \n",
    "            if done == True: #game ended\n",
    "                for _ in range(10):\n",
    "                    agent.remember(state, nextState, action, reward, done, stepCounter)\n",
    "                print(\"breaking\")\n",
    "                break\n",
    "            ########\n",
    "            state = nextState\n",
    "            stepCounter += 1\n",
    "            epReward += reward\n",
    "\n",
    "        #post episode\n",
    "        if stepCounter != 0:\n",
    "            print(\"Avg Frame-Rate: \", 1/((time.time()-episodeTime)/stepCounter))\n",
    "        plotX.append(epReward)\n",
    "        print(epReward)\n",
    "        agent.learn()\n",
    "\n",
    "\n",
    "       \n",
    "        if i % 20 == 0:\n",
    "            agent.model.save_weights(\"DinoGameSpeed4.h5\")\n",
    "            print( \"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next section plots the AI's total reward and the Nueral net learning loss for the AI as a function of the episode number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(plotX)),plotX) \n",
    "plt.show()\n",
    "plt.plot(range(len(agent.loss)), agent.loss) \n",
    "plt.show() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
